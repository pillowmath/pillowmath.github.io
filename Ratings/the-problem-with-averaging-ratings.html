<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="../github-markdown.css">
<style>
	.markdown-body {
		box-sizing: border-box;
		min-width: 200px;
		max-width: 980px;
		margin: 0 auto;
		padding: 45px;
	}

	@media (max-width: 767px) {
		.markdown-body {
			padding: 15px;
		}
	}
	
	.aligncenter {
	    text-align: center;
	}
	
</style>
<head>
	<title>Pillowmath</title>
	<link rel="shortcut icon" type="image/x-icon" href="../favicon.ico?" >
	<!-- Google Tag Manager -->
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-MGGSG76J');</script>
	<!-- End Google Tag Manager -->
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-DQE6L5ZQWV"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-DQE6L5ZQWV');
	</script>
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-106127486-1', 'auto');
	  ga('send', 'pageview');

	</script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
	<!-- Google Tag Manager (noscript) -->
	<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MGGSG76J"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->
<article class="markdown-body">
	<h1>The Problem With Averaging Ratings (And How to Fix It)</h1>
	<h2>Personal rating scales</h2>
	<p>Here’s a common situation: We want to compare a number of items—like <a href=https://www.imdb.com/chart/top/?ref_=nv_mv_250>movies</a>, <a href=https://www.yelp.com/search?find_desc=Restaurants>restaurants</a>, or <a href=https://pillowmath.github.io/Ratings/job-applicant-ratings.html>job applicants</a>—to see which ones we prefer to others. So we have people provide numerical ratings for each item—like a number between 1 and 10—and we compare the average ratings for each item. This is an approach that goes back <a href=https://psycnet.apa.org/record/1933-01885-001>almost a hundred years</a>, but there’s a fundamental issue: people rate according to different <i>personal rating scales</i>.</p>
	<p>For example, suppose we have two people, Alice and Bob, rating movies.</p>
	
	<p class="aligncenter"><img style="max-width: 500px; height: auto; " src="Alice-bob-rating-matrix.png"></img></p>
	
	<p>Alice tends to give ratings between 2 and 9 (out of 10), and Bob tends to give ratings between 8 and 10.</p>
	
	<p class="aligncenter"><img style="max-width: 600px; height: auto; " src="Alice-bob-rating-scales.png"></img></p>
	
	<p>If Alice and Bob both rate a movie 8, then these numbers mean very different things! Alice loved the movie, but Bob hated it—and the average treats these two ratings as the same.</p>
	
	<p>Averaging takes the numbers at face value. A score of 8 is considered to be literally twice as good as a 4, and the difference between 7 and 8 is assumed to be the same as the difference between 9 and 10. But this doesn't capture the difference in meaning of Alice and Bob's ratings.</p>
	
	<p>Personal rating scales can lead to unfair comparisons, as well. If Alice and Bob rate different job applicants, Bob’s high ratings give an unfair advantage to anyone he rates. Simply averaging ratings forces us to interpret these numbers in a way which is divorced from their context.
</p>
	<h2>Looking at the data</h2>
	<p>If we take a closer look at the data, we can get a hint as to where we're going wrong. When we average the ratings for a particular item, we only look at one column of the rating matrix at a time.</p>
	
	<p class="aligncenter"><img style="max-width: 500px; height: auto; " src="Rating-average-column.png"></img></p>
	
	<p>However, we have a greater wealth of information available. Looking at each row of the rating matrix provides context for that person's ratings. When Alice rates multiple items, we can learn her personal rating scale via her <i>empirical rating distribution</i>.</p>
	
	<p class="aligncenter"><img style="max-width: 500px; height: auto; " src="Alice-bob-empirical-rating-dists.png"></img></p>
	
	Now let's see how we can use this insight to average the ratings in a smarter way.
		
	<h2>A common scale</h2>
	<p>If we can't average Alice's and Bob's ratings because they correspond to different rating scales, then we should first convert the scores to corresponding ratings on a common/consensus scale to make them comparable. In our example, Alice's 8 might be converted to a consensus 9, while Bob's 8 might be converted to a consensus 2. But how do we determine a common rating scale? Taking a step back: when we convert someone’s ratings—say, Alice’s—to the common scale, we’re effectively reshaping her personal rating distribution.</p>
	
	<p class="aligncenter"><img style="max-width: 700px; height: auto; " src="Optimal-transport-diagram.png"></img></p>
	
	<p>This reshaping is known as an <i>optimal transport map</i>, and we can measure how much it changes her original scale. So a natural goal is to find a common scale that changes everyone’s personal scales as little as possible. This gives a kind of average of everyone’s personal rating scales, called a <i>Wasserstein barycenter</i>.</p>
	
	<p class="aligncenter"><img style="max-width: 875px; height: auto; " src="Converting-to-consensus-scale.png"></img></p>
	
	<p>Once we convert the ratings for an item to a common scale, we can compare and average them. This gives us a new method of averaging ratings, <b>the rating estimator</b>. </p>
	
	<h2>Benefits of the rating estimator</h2>
	<p>The rating estimator has a number of benefits over the standard average:</p>
	
	<h3>Corrects rater bias in sparse data</h3>
	<p>Suppose that Alice and Bob rate different job applicants. As mentioned before, when we use the standard average, Bob's high ratings give an unfair advantage to anyone he rates. However, by converting Bob's ratings to the common scale, the rating estimator accounts for his right-shifted rating scale, leading to a fairer comparison.</p>
	
	<h3>Meaningful differences between ratings</h3>
	<p>Because people interpret rating scales differently, a difference between, say, 7 and 8 might not mean the same thing across raters. This makes it hard to interpret differences between average ratings in a meaningful way. The rating estimator addresses this by reporting scores on a shared scale—the Wasserstein barycenter—so that differences between scores are directly comparable. Moreover, using the Wasserstein barycenter preserves the overall scale of the original data. So if ratings range from 1 to 5 stars and users tend to use that full range, the rating estimator scores will span that same range, rather than being compressed into a narrow band like 3.5 to 4.5, which often happens with simple averages.</p>
	
	<h3>Highly ranked items are preferred by raters</h3>
	Average ratings can be inflated by generous raters who tend to give high scores across the board (such as new users on a rating website). By contrast, the rating estimator highlights items that each rater ranked at the top of their list. As a result, the estimator's top-rated items are more likely to be preferred over lower-ranked ones by more raters.
	
	<h3>Theoretical guarantees</h3>
	Under non-parametric assumptions—that is, without assuming what the ratings look like—the rating estimator <i>provably</i> outperforms the average in estimating the quality of items (for details, see <a href=https://arxiv.org/abs/2410.00865>my paper</a> on the rating estimator).
	
	<h2>Analyses of real-world rating data</h2>
	<p>Coming soon! For now, check out Section 8 of <a href=https://arxiv.org/abs/2410.00865>my paper</a> on the rating estimator for an analysis of anime ratings from <a href=https://myanimelist.net/topanime.php>MyAnimeList</a>. And if you want to try out the rating estimator on your own rating data, <a href=https://github.com/pillowmath/Rating-Estimator>here's</a> a Python implementation, as well. Here are some highlights from the analysis in the paper:
		<ul>
			<li>MyAnimeList users prefer the top anime recommended by the rating estimator to the top anime recommended by the standard average.</li>
			<li>On average, the difference in item rankings (not ratings) between the average and the rating estimator is 11% of the total number of items.</li>
			<li>The rating estimator can be used to quantify how similar people's ratings are to each other. It turns out that users of MyAnimeList have relatively similar personal rating scales, but their preferences differ heavily!</li>
		</ul>
		</p>
	
	<h2>Notes on applying the rating estimator</h2>
	<p>If you want to try out the rating estimator on your own rating data, <a href=https://github.com/pillowmath/Rating-Estimator>here's</a> a Python implementation; just plug your rating matrix into rating_estimator.py! Here are a few things to keep in mind:
		<ul>
			<li>The rating estimator requires multiple ratings from each person so that we can learn their personal rating scale. If you're only rating a single item, the rating estimator will give the same answer as the usual average.</li>
			<li>The rating estimator is appropriate for data where people can rate on different personal rating scales. It doesn't make sense to use with data where people are forced to rate on the same scale, such as this:
			<p class="aligncenter"><img style="max-width: 800px; height: auto; " src="Labeled-rating-scale.png"></img></p>
			Similarly, if the items are incomparable, it doesn't make sense to combine a person's ratings into one rating scale.</li>
			<li>The rating estimator converts each person's ratings to the consensus scale by matching quantiles, but when calculating quantiles, the method you use can produce a different result. The code I've provided uses the CDF and inverse CDF to calculate quantiles, but this is "optimistic," in the sense that it picks the higher values at discontinuity points. If the results are too right-shifted for your context, consider trying out a different method for performing quantile calculations, such as averaged CDFs/inverse CDFs or any of the ones <a href=https://numpy.org/doc/2.1/reference/generated/numpy.quantile.html>here</a>.</li>
			<li>When applying the rating estimator to a database you maintain (such as ratings on an app/website), consider updating the rating estimator scores using a <a href=https://en.wikipedia.org/wiki/Moving_average#Cumulative_average>cumulative moving average</a> to avoid periodically performing a large computation. Computations of the Wasserstein barycenter and the ratings themselves can both be simplified in this way.</li>
			<li>With very large datasets, the rating matrix may be too large to fit into memory. My implementation has a <a href=https://github.com/pillowmath/Rating-Estimator/blob/main/rating_estimator_sparse.py>sparse matrix option</a> to get around this issue; just load in your rating matrix in SciPy CSR format as an .npz file, along with a .csv listing your item ids (in the same order as the columns of your matrix).</li>
		</ul>
		</p>
	
	
	<h2>Mathematical description of the rating estimator</h2>
	<p>In mathematical terms, the Wasserstein barycenter has a simple formula. If everyone's personal rating distributions have <a href=https://en.wikipedia.org/wiki/Cumulative_distribution_function>cumulative distribution functions</a> \( F_1, \dots, F_n \) (so the <a href=https://en.wikipedia.org/wiki/Quantile_function>quantile functions</a> are \( F_1^{-1},\dots,F_n^{-1} \)), then the consensus scale \( \widehat \mu \) has quantile function \[ F_{\widehat \mu}^{-1} = \frac{1}{n} \sum_{i=1}^n F_i^{-1}, \] and the optimal transport map converting person \( k \)'s ratings to the common scale is \[F_{\widehat \mu}^{-1} \circ F_k = \frac{1}{n} \sum_{i=1}^n F_i^{-1} \circ F_k. \]</p>
	
	<p>Here is the formula for the rating estimator. It has two steps, the first of which we've already described.</p>
	
	<p><b>Step 1.</b> (Primitive ratings): Given an item with ratings \( r_1,...,r_n \), define the <i>primitive aggregate rating</i> for that item as \[ \begin{align*} R_0(r_1,\dots,r_n) &:= \frac{1}{n} \sum_{k=1}^n F_{\widehat \mu}^{-1} \circ F_k(r_k) \\ &= \frac{1}{n^2} \sum_{i=1}^n \sum_{k=1}^n F_i^{-1} \circ F_k(r_k). \end{align*} \] Calculate the primitive rating for all items to obtain the distribution \( \nu \) of primitive
ratings for all items.</p>

	<p><b>Step 2.</b> (Final rescaling): Adjust each primitive rating \( r \) by reporting the aggregate
rating for that item as \[ \begin{align*} R(r) &:= F_{\widehat \mu}^{-1} \circ F_\nu(r) \\ &= \frac{1}{n} \sum_{i=1}^n F_i^{-1} \circ F_\nu(r). \end{align*} \]</p>
	
	<p>Step 2, applied to a primitive rating for each item, yields the rating estimator score for each item. Why do we need this second step? It turns out that if people disagree on preferences, i.e. which items are better than others, then \( R_0 \) outputs a distribution of \( \nu \) which is different from the consensus scale \( \widehat \mu \). Step 2 rescales the result to fit the consensus scale, without changing the ordering of the values that \( R_0 \) gave us.</p>
	
	<p>For more details, including an application to anime ratings from <a href=https://myanimelist.net/topanime.php>MyAnimeList</a>, check out <a href=https://arxiv.org/abs/2410.00865>my paper</a> on the rating estimator! And if you want to try out the rating estimator on your own rating data, <a href=https://github.com/pillowmath/Rating-Estimator>here's</a> a Python implementation, as well. Here are some more highlights from the paper:
		<ul>
			<li>If everyone ranks all the items in the same order, the rating estimator is the same as the standard average.</li>
			<li>The rating estimator achieves the optimal rate for the number of raters needed for accurate judgments.</li>
			<li>The rating estimator applies beyond rating systems; it can be used in a number of settings where we observe distorted views of some underlying average behavior and aim to recover that behavior.</li>
		</ul>
		</p>
	
	</article>
</body>